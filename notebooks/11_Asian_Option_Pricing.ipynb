{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 11. Asian Option Pricing using Neural Path Integral\n",
                "\n",
                "This notebook demonstrates **Neural Importance Sampling** for **Arithmetic Asian Call Options**.\n",
                "\n",
                "## Asian Option\n",
                "The payoff depends on the arithmetic average of the asset price over the path:\n",
                "$$ \\text{Payoff} = \\max(A_T - K, 0) $$\n",
                "where $A_T = \\frac{1}{T} \\int_0^T S_t dt$.\n",
                "\n",
                "## Objective\n",
                "Since the payoff depends on the running average $A_t$, the Neural Control Policy needs to know $A_t$ in addition to $S_t$ and $v_t$.\n",
                "We have updated `DriftNet` to take $(t, S_t, v_t, A_t)$ as input."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import sys\n",
                "sys.path.append('..')\n",
                "\n",
                "from src.physics_engine import MarketSimulator\n",
                "from src.neural_engine import NeuralImportanceSampler\n",
                "\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment & Option Parameters\n",
                "We use the **Heston Model** which adds stochastic volatility complexity.\n",
                "\n",
                "**Bias Correction**: We reduce `dt` to 0.002 (500 steps) to handle the strong control force utilized by the network."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Option Parameters (Arithmetic Asian Call)\n",
                "S0 = 100.0\n",
                "K = 100.0     # ATM\n",
                "T = 1.0\n",
                "r = 0.05\n",
                "dt = 0.002    # 500 steps (Finer discretization)\n",
                "\n",
                "# Heston Parameters\n",
                "kappa = 2.0\n",
                "theta = 0.04\n",
                "xi = 0.3\n",
                "rho = -0.7\n",
                "\n",
                "sim = MarketSimulator(mu=r, kappa=kappa, theta=theta, xi=xi, rho=rho, device=device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Standard Monte Carlo (Benchmark)\n",
                "We run a high-precision MC simulation to establish the ground truth price and variance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Running Standard Monte Carlo...\")\n",
                "N_mc = 100000\n",
                "batch_size = 10000\n",
                "prices_mc = []\n",
                "\n",
                "with torch.no_grad():\n",
                "    for _ in range(N_mc // batch_size):\n",
                "        # simulate_controlled returns tuple: (S, v, log_w, hit, running_int_S)\n",
                "        S, _, _, _, running_int_S = sim.simulate_controlled(\n",
                "            S0=S0, v0=sim.theta, T=T, dt=dt, num_paths=batch_size, \n",
                "            control_fn=None  # Standard MC\n",
                "        )\n",
                "        \n",
                "        # Calculate Average Price A_T\n",
                "        A_T = running_int_S / T\n",
                "        \n",
                "        # Asian Call Payoff\n",
                "        payoff = torch.maximum(A_T - K, torch.tensor(0.0, device=device))\n",
                "        \n",
                "        discounted_payoff = torch.exp(torch.tensor(-r*T, device=device)) * payoff\n",
                "        prices_mc.extend(discounted_payoff.cpu().numpy())\n",
                "\n",
                "mean_mc = np.mean(prices_mc)\n",
                "std_mc = np.std(prices_mc) / np.sqrt(len(prices_mc))\n",
                "sigma_mc = np.std(prices_mc)  # Std per path\n",
                "print(f\"Standard MC Asian Price: {mean_mc:.4f} +/- {1.96*std_mc:.4f} (95% CI)\")\n",
                "print(f\"Standard MC StdDev (per path): {sigma_mc:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training Neural Control (Variance Reduction)\n",
                "We train the network to minimize the second moment of the weighted payoff.\n",
                "For Asian Call, OTM paths have 0 payoff (0 variance). ITM paths contribute variance.\n",
                "The optimal control should stabilize the average price."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sampler = NeuralImportanceSampler(sim, hidden_dim=64)\n",
                "# Learning Rate - conservative to avoid instability\n",
                "optimizer = torch.optim.Adam(sampler.control_net.parameters(), lr=0.001)\n",
                "\n",
                "losses = []\n",
                "prices_history = []\n",
                "\n",
                "print(\"Training Neural Control for Asian Option...\")\n",
                "for i in range(301):\n",
                "    sampler.control_net.train()\n",
                "    optimizer.zero_grad()\n",
                "    \n",
                "    # Training also uses finer steps\n",
                "    dt_train = dt \n",
                "    control_fn = sampler.get_control_fn()\n",
                "    \n",
                "    # Run Simulation with Control\n",
                "    S, _, log_weights, _, running_int_S = sim.simulate_controlled(\n",
                "        S0=S0, v0=sim.theta, T=T, dt=dt_train, num_paths=3000, \n",
                "        control_fn=control_fn\n",
                "    )\n",
                "    \n",
                "    # Asian Payoff\n",
                "    A_T = running_int_S / T\n",
                "    payoff = torch.maximum(A_T - K, torch.tensor(0.0, device=device))\n",
                "    \n",
                "    # Weighted Payoff\n",
                "    weighted_payoff = payoff * torch.exp(log_weights)\n",
                "    \n",
                "    # Loss: Second Moment\n",
                "    loss = torch.mean(weighted_payoff ** 2)\n",
                "    \n",
                "    loss.backward()\n",
                "    # Clip gradients for stability\n",
                "    torch.nn.utils.clip_grad_norm_(sampler.control_net.parameters(), max_norm=1.0)\n",
                "    optimizer.step()\n",
                "    \n",
                "    # Monitor Price Estimate (Discounted)\n",
                "    price_est = torch.mean(weighted_payoff).item() * np.exp(-r*T)\n",
                "    \n",
                "    losses.append(loss.item())\n",
                "    prices_history.append(price_est)\n",
                "    \n",
                "    if i % 30 == 0:\n",
                "        print(f\"Iter {i}: Loss = {loss.item():.6f}, Price Est = {price_est:.4f}\")\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(losses)\n",
                "plt.title('Loss (Second Moment)')\n",
                "plt.yscale('log')\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(prices_history)\n",
                "plt.axhline(mean_mc, color='r', linestyle='--', label='MC Benchmark')\n",
                "plt.title('Asian Price Convergence')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Final Verification\n",
                "Compare Variance of Standard MC vs Neural IS."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "N_eval = 50000\n",
                "\n",
                "# 1. Neural IS\n",
                "with torch.no_grad():\n",
                "    sampler.control_net.eval()\n",
                "    control_fn = sampler.get_control_fn()\n",
                "    S, _, log_w, _, running_int_S = sim.simulate_controlled(\n",
                "        S0=S0, v0=sim.theta, T=T, dt=dt, num_paths=N_eval, control_fn=control_fn\n",
                "    )\n",
                "    \n",
                "    A_T = running_int_S / T\n",
                "    payoff = torch.maximum(A_T - K, torch.tensor(0.0, device=device))\n",
                "    weighted_payoff = payoff * torch.exp(log_w) * torch.exp(torch.tensor(-r*T, device=device))\n",
                "    \n",
                "    neural_mean = torch.mean(weighted_payoff).item()\n",
                "    neural_std = torch.std(weighted_payoff).item()\n",
                "\n",
                "# 2. Standard MC (Same N)\n",
                "with torch.no_grad():\n",
                "    S, _, _, _, running_int_S = sim.simulate_controlled(\n",
                "        S0=S0, v0=sim.theta, T=T, dt=dt, num_paths=N_eval, control_fn=None\n",
                "    )\n",
                "    \n",
                "    A_T = running_int_S / T\n",
                "    payoff = torch.maximum(A_T - K, torch.tensor(0.0, device=device))\n",
                "    val = payoff * torch.exp(torch.tensor(-r*T, device=device))\n",
                "    \n",
                "    mc_mean = torch.mean(val).item()\n",
                "    mc_std = torch.std(val).item()\n",
                "\n",
                "print(f\"Results (N={N_eval}):\")\n",
                "print(f\"Standard MC: {mc_mean:.4f} +/- {1.96 * mc_std / np.sqrt(N_eval):.4f} (StdDev: {mc_std:.4f})\")\n",
                "print(f\"Neural IS  : {neural_mean:.4f} +/- {1.96 * neural_std / np.sqrt(N_eval):.4f} (StdDev: {neural_std:.4f})\")\n",
                "\n",
                "if neural_std > 1e-9:\n",
                "    ratio = (mc_std / neural_std)**2\n",
                "    print(f\"\\nVariance Reduction Ratio: {ratio:.2f}x\")\n",
                "else:\n",
                "    print(\"\\nVariance Reduction Ratio: Undefined (Neural StdDev ~= 0)\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
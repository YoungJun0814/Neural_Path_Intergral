{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 10. Barrier Option Pricing using Neural Path Integral\n",
                "\n",
                "This notebook demonstrates the **Variance Reduction** capability of the Neural Path Integral method for **Down-and-Out Put Options**.\n",
                "\n",
                "## Theory\n",
                "We use **Neural Importance Sampling** to guide option paths away from the barrier (or towards specific regions) to minimize the variance of the estimator.\n",
                "\n",
                "$$ \\text{Loss} = \\text{Var} \\left[ \\text{Payoff}(S^u) \\cdot e^{-\\text{Action}(u)} \\right] $$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import sys\n",
                "sys.path.append('..')\n",
                "\n",
                "from src.physics_engine import MarketSimulator\n",
                "from src.neural_engine import NeuralImportanceSampler\n",
                "from src.quantum_solver import PathIntegralSolver\n",
                "\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Environment\n",
                "We define a Heston model environment and a specific Down-and-Out Put option."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Parameters\n",
                "S0 = 100.0\n",
                "K = 90.0      # Strike\n",
                "B = 80.0      # Barrier (Down-and-Out)\n",
                "T = 1.0       # Maturity\n",
                "r = 0.05      # Risk-free rate\n",
                "dt = 0.01\n",
                "\n",
                "# Heston Parameters\n",
                "kappa = 2.0\n",
                "theta = 0.04\n",
                "xi = 0.3\n",
                "rho = -0.7\n",
                "\n",
                "sim = MarketSimulator(mu=r, kappa=kappa, theta=theta, xi=xi, rho=rho, device=device)\n",
                "solver = PathIntegralSolver(sim)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Standard Monte Carlo (Benchmark)\n",
                "We run a large-scale simulation to get a \"ground truth\" benchmark."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Running Standard Monte Carlo...\")\n",
                "N_mc = 100000\n",
                "batch_size = 10000\n",
                "prices_mc = []\n",
                "\n",
                "with torch.no_grad():\n",
                "    for _ in range(N_mc // batch_size):\n",
                "        S, _, _, hit = sim.simulate_controlled(S0, theta, T, dt, batch_size, \n",
                "                                               barrier_level=B, barrier_type='down-out')\n",
                "        S_final = S[:, -1]\n",
                "        # Put Payoff\n",
                "        payoff = torch.maximum(torch.tensor(K, device=device) - S_final, torch.tensor(0.0, device=device))\n",
                "        # Knock-out\n",
                "        payoff = payoff * (~hit).float()\n",
                "        \n",
                "        discounted_payoff = torch.exp(torch.tensor(-r*T, device=device)) * payoff\n",
                "        prices_mc.extend(discounted_payoff.cpu().numpy())\n",
                "\n",
                "mean_mc = np.mean(prices_mc)\n",
                "std_mc = np.std(prices_mc) / np.sqrt(len(prices_mc))\n",
                "print(f\"Standard MC Price: {mean_mc:.4f} +/- {1.96*std_mc:.4f} (95% CI)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training Neural Importance Sampler\n",
                "We train the neural network to minimize the variance of the weighted payoff.\n",
                "Ideally, if variance -> 0, we found the optimal control."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sampler = NeuralImportanceSampler(sim, hidden_dim=64)\n",
                "# LR reduced to 0.0005 for stability with Bound=0.2\n",
                "optimizer = torch.optim.Adam(sampler.control_net.parameters(), lr=0.0005)\n",
                "\n",
                "losses = []\n",
                "prices_history = []\n",
                "\n",
                "print(\"Training Neural Control...\")\n",
                "for i in range(201):\n",
                "    # Manual unroll of train_step to add gradient clipping\n",
                "    sampler.control_net.train()\n",
                "    optimizer.zero_grad()\n",
                "    \n",
                "    dt_train = min(0.01, T / 100.0)\n",
                "    control_fn = sampler.get_control_fn()\n",
                "    \n",
                "    S_paths, _, log_weights, barrier_hit = sampler.sim.simulate_controlled(\n",
                "        S0=S0, v0=sim.theta, T=T, dt=dt_train, num_paths=5000,  # Increased batch size for smoother gradients\n",
                "        control_fn=control_fn, barrier_level=B, barrier_type='down-out'\n",
                "    )\n",
                "    \n",
                "    S_final = S_paths[:, -1]\n",
                "    payoffs = torch.maximum(torch.tensor(K, device=device) - S_final, \n",
                "                            torch.tensor(0.0, device=device))\n",
                "    if barrier_hit is not None:\n",
                "         payoffs = payoffs * (~barrier_hit).float()\n",
                "    \n",
                "    weighted_payoffs = payoffs * torch.exp(log_weights)\n",
                "    loss = torch.mean(weighted_payoffs ** 2)\n",
                "    \n",
                "    loss.backward()\n",
                "    \n",
                "    # Gradient Clipping\n",
                "    torch.nn.utils.clip_grad_norm_(sampler.control_net.parameters(), max_norm=1.0)\n",
                "    \n",
                "    optimizer.step()\n",
                "    \n",
                "    price_est = torch.mean(weighted_payoffs).item()\n",
                "    \n",
                "    losses.append(loss.item())\n",
                "    prices_history.append(price_est)\n",
                "    \n",
                "    if i % 20 == 0:\n",
                "        print(f\"Iter {i}: Loss (Variance) = {loss.item():.6f}, Price Est = {price_est:.4f}\")\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(losses)\n",
                "plt.title('Second Moment (Loss)')\n",
                "plt.yscale('log')\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(prices_history)\n",
                "plt.axhline(mean_mc, color='r', linestyle='--', label='MC Benchmark')\n",
                "plt.title('Price Convergence')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Final Evaluation\n",
                "We compare the variance of Standard MC vs Neural MC using the same number of paths."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "N_eval = 5000\n",
                "\n",
                "# 1. Neural IS\n",
                "with torch.no_grad():\n",
                "    sampler.control_net.eval()\n",
                "    control_fn = sampler.get_control_fn()\n",
                "    S, _, log_w, hit = sim.simulate_controlled(S0, theta, T, dt, N_eval, \n",
                "                                               control_fn=control_fn, barrier_level=B, barrier_type='down-out')\n",
                "    S_final = S[:, -1]\n",
                "    payoff = torch.maximum(torch.tensor(K, device=device) - S_final, torch.tensor(0.0, device=device))\n",
                "    payoff = payoff * (~hit).float()\n",
                "    weighted_payoff = payoff * torch.exp(log_w) * torch.exp(torch.tensor(-r*T, device=device))\n",
                "    \n",
                "    neural_mean = torch.mean(weighted_payoff).item()\n",
                "    neural_std = torch.std(weighted_payoff).item()\n",
                "\n",
                "# Standard MC (re-run with small N)\n",
                "with torch.no_grad():\n",
                "    S, _, _, hit = sim.simulate_controlled(S0, theta, T, dt, N_eval, \n",
                "                                               barrier_level=B, barrier_type='down-out')\n",
                "    S_final = S[:, -1]\n",
                "    payoff = torch.maximum(torch.tensor(K, device=device) - S_final, torch.tensor(0.0, device=device))\n",
                "    payoff = payoff * (~hit).float()\n",
                "    val = payoff * torch.exp(torch.tensor(-r*T, device=device))\n",
                "    mc_mean = torch.mean(val).item()\n",
                "    mc_std = torch.std(val).item()\n",
                "\n",
                "print(f\"Results (N={N_eval}):\")\n",
                "print(f\"Standard MC: {mc_mean:.4f} +/- {1.96 * mc_std / np.sqrt(N_eval):.4f} (StdDev: {mc_std:.4f})\")\n",
                "print(f\"Neural IS  : {neural_mean:.4f} +/- {1.96 * neural_std / np.sqrt(N_eval):.4f} (StdDev: {neural_std:.4f})\")\n",
                "\n",
                "if neural_std > 1e-9:\n",
                "    print(f\"\\nVariance Reduction Ratio: {mc_std**2 / neural_std**2:.2f}x\")\n",
                "else:\n",
                "    print(f\"\\nVariance Reduction Ratio: Undefined (Neural StdDev ~= 0). Suggests bias/collapse if Neural Mean != MC Mean.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}